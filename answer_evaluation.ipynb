{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from importlib import reload\n",
    "import logging\n",
    "from logging.config import dictConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nlp_utils import read_dataset\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables/setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "reload(logging)\n",
    "#logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%I:%M:%S')\n",
    "dictConfig({\n",
    "    'version': 1,\n",
    "    'formatters': {'default': {\n",
    "        'format': '[%(asctime)s] %(levelname)s: %(message)s',\n",
    "    }},\n",
    "    'handlers': {'log_file': {\n",
    "        'class': 'logging.handlers.RotatingFileHandler',\n",
    "        'filename': 'nlp_answer_evaluation.log',\n",
    "        'formatter': 'default',\n",
    "        'maxBytes': 1024 * 1024 * 20,\n",
    "        'backupCount': 3\n",
    "        },\n",
    "        'log_stream': {\n",
    "            'class': 'logging.StreamHandler',\n",
    "            'formatter': 'default'\n",
    "        }\n",
    "    },\n",
    "    'root': {\n",
    "        'level': 'INFO',\n",
    "        'handlers': ['log_file', 'log_stream']\n",
    "    }\n",
    "})\n",
    "\n",
    "\n",
    "np.random.seed(1) # Set seed value for numpy random\n",
    "stemmer = LancasterStemmer() # Declare stemmer object\n",
    "download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) # Set stop words for english\n",
    "dataset_dir = 'dataset' # Set dataset directory\n",
    "models_dir = 'models' # Set models directory to save/load\n",
    "vectorizer_dir = 'vectorizer' # Set vectorizer directory to save/load\n",
    "file_cache = False # Set flag to use file cache or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(directory, filename, data):\n",
    "    '''\n",
    "    Saves an object to file using joblib.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory.\n",
    "        filename (str): Name of file.\n",
    "        data : Object/data to be saved.\n",
    "    '''\n",
    "    file_to_save = os.path.join(directory, filename)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    joblib.dump(data, file_to_save)\n",
    "    logging.debug('Saved data to file {}'.format(file_to_save))\n",
    "\n",
    "def load_object(directory, filename):\n",
    "    '''\n",
    "    Loads a model from file using joblib.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory.\n",
    "        filename (str): Name of file.\n",
    "    \n",
    "    Returns:\n",
    "        data : Object/data that's loaded from given file.\n",
    "    '''\n",
    "    file_to_load = os.path.join(directory, filename)\n",
    "    data= joblib.load(file_to_load)\n",
    "    logging.debug('Loaded file {}'.format(file_to_load))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    datasets = read_dataset(dataset_dir) # Read the datasets from directory\n",
    "except Exception as e:\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(sentence_list):\n",
    "    '''\n",
    "    Performs stemming on the text dataset.\n",
    "\n",
    "    Args:\n",
    "        sentence_list (list): List of the sentences.\n",
    "\n",
    "    Returns:\n",
    "        stemmed_sentences (list): Sentences after stemming the words.\n",
    "    '''\n",
    "    stemmed_sentences = []\n",
    "    for sentence in sentence_list:\n",
    "        sentence = sentence.lower() # Lowercase the words\n",
    "        sentence = re.sub(r'\\W', ' ', sentence) # Remove \n",
    "        sentence = re.sub(r'\\s+', ' ', sentence, flags=re.I) # Remove extra spaces\n",
    "        sentence = sentence.split() # Split the sentence\n",
    "        stemmed_tokens = [stemmer.stem(word) for word in sentence] # Perform stemming on the words\n",
    "        stemmed_sentences.append(' '.join(stemmed_tokens)) # Join the stemmed words back\n",
    "    return stemmed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data (vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sentence_vectors(question_id, sentence_list):\n",
    "    '''\n",
    "    Creates a vector representation of the sentences using bag of words approach.\n",
    "\n",
    "    Args:\n",
    "        question_id (str): ID of the Question\n",
    "        sentence_list (list): List of the sentences.\n",
    "\n",
    "    Returns:\n",
    "        sentence_vector_list (list): Vectors for all the sentences.\n",
    "    '''\n",
    "    # Initialize vectorizer to preprocess, tokenize and vectorize dataset\n",
    "    count_vector = CountVectorizer(binary=True, stop_words=stop_words)\n",
    "    count_vector = count_vector.fit(sentence_list) # Fit the sentence list on vectorizer\n",
    "    try:\n",
    "        save_object(vectorizer_dir, question_id, count_vector) # Save the vectorizer to file\n",
    "    except:\n",
    "        logging.error('Unable to save vectorizer to file for question {}'.format(question_id))\n",
    "    sentence_vector_list = count_vector.transform(sentence_list) # Generate vectors for sentences\n",
    "    return sentence_vector_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, test_dataset_size= 0.2):\n",
    "    '''\n",
    "    Trains model using the dataset and returns the trained model.\n",
    "\n",
    "    Args:\n",
    "        dataset (dict): The dataset to process.\n",
    "        test_dataset_size (float): Size of test dataset while splitting (0 to 1).\n",
    "\n",
    "    Returns:\n",
    "        answer_classifier (model): The model that's trained using dataset (using SVM classifier).\n",
    "        score (float): Accuracy score for the model (0 to 1).\n",
    "    '''\n",
    "    answers = [dataset['reference_answer']['text']] # Fetch the reference answer\n",
    "    answers.extend(dataset['answers']['sentences']) # Add all answers to the input\n",
    "    output_data = ['correct'] # Set the class for reference answer as correct\n",
    "    output_data.extend(dataset['answers']['classes']) # Add all other values to output\n",
    "    cleaned_answers = preprocess_dataset(answers) # Perform preprocessing of dataset\n",
    "    answers_matrix = build_sentence_vectors(dataset['question']['id'],cleaned_answers) # Transform the dataset\n",
    "    # Split the dataset to train and split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(answers_matrix, output_data,\n",
    "                                                        test_size= test_dataset_size, random_state=0)\n",
    "    answer_classifier = SVC(kernel='linear').fit(x_train, y_train) # Fit the data on SVM classifier\n",
    "    y_pred = answer_classifier.predict(x_test) # Predict the classes for test dataset\n",
    "    score = accuracy_score(y_test, y_pred) # Score the model\n",
    "    logging.debug('Score for dataset with question id {} is {}'.format(dataset['question']['id'], score))\n",
    "    return answer_classifier, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate model for datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-07-09 12:58:07,834] INFO: The average accuracy over all datasets trained now is 82.51851851851852\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = []\n",
    "for dataset in datasets:\n",
    "    dataset_name = dataset['question']['id'] # Get question ID\n",
    "    model_file = os.path.join(models_dir, dataset_name)\n",
    "    if file_cache and os.path.isfile(model_file): # Check if cache is enabled and file exists\n",
    "        continue\n",
    "    model, score = train(dataset) # Train the model using dataset\n",
    "    save_object(models_dir, dataset_name, model) # Save the model\n",
    "    accuracy_list.append(score)\n",
    "\n",
    "if len(accuracy_list) > 0:\n",
    "    logging.info(\"The average accuracy over all datasets trained now is {}\".format(np.mean(accuracy_list)*100))\n",
    "else:\n",
    "    logging.info(\"No new dataset to process! Please set file_cache to False to reprocess the datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(question_id, answer):\n",
    "    '''\n",
    "    Classify an answer as correct or incorrect, given its question ID.\n",
    "\n",
    "    Args:\n",
    "        question_id (str): The ID of the question for which answer is provided.\n",
    "        answer (str): Answer to be classified.\n",
    "\n",
    "    Returns:\n",
    "        predicted_class (str): Predicted class of the answer (correct or incorrect).\n",
    "        None : If there was any error.\n",
    "    '''\n",
    "    model_to_load = os.path.join(models_dir, question_id)\n",
    "    vectorizer_to_load = os.path.join(vectorizer_dir, question_id)\n",
    "    if os.path.isfile(model_to_load) and os.path.isfile(vectorizer_to_load):\n",
    "        try:\n",
    "            model = load_object(models_dir, question_id)\n",
    "            cleaned_answer = preprocess_dataset([answer])\n",
    "            vectorizer = load_object(vectorizer_dir, question_id)\n",
    "            answer_vector = vectorizer.transform(cleaned_answer)\n",
    "            predicted_class = model.predict(answer_vector)[0]\n",
    "            return predicted_class\n",
    "        except:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "question_id = str(input('Please enter Question ID :')).lower()\n",
    "answer = str(input('Please enter answer : '))\n",
    "answer_class = classify(question_id, answer)\n",
    "if answer_class is None:\n",
    "    print(\"Unable to load model/vectorizer data. Please check if the question ID is valid.\")\n",
    "else:\n",
    "    print(\"The answer is predicted as : {}\".format(answer_class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
